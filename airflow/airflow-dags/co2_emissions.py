"""
Airflow DAG to trigger Databricks Serverless Job
COâ‚‚ Emissions ETL: Bronze â†’ Silver â†’ Gold
"""

from airflow import DAG
from airflow.operators.email import EmailOperator
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator
from airflow.providers.databricks.hooks.databricks_sql import DatabricksSqlHook
from airflow.operators.python import PythonOperator, ShortCircuitOperator
from datetime import datetime, timedelta
import logging

# ------------------------------------------------
# Logger & Helper Functions
# ------------------------------------------------
logger = logging.getLogger(__name__)

def validate_inputs(**context):
    logger.info("âœ… Validating source files...")
    return True

def check_outputs(**context):
    logger.info("âœ… Checking output quality...")
    return {"status": "success", "records": 7000}

def check_emission_alerts(**context):
    """
    Checks if ALERT rows exist in gold_emission_alerts table.
    Returns True â†’ continue pipeline
    Returns False â†’ stop email task
    """
    logger.info("ðŸ” Checking COâ‚‚ emission alerts in GOLD table...")

    hook = DatabricksSqlHook(
        databricks_conn_id="databricks_default"
    )

    sql = """
      SELECT COUNT(*)
    FROM co2_emissions.default.gold_emission_alerts
    WHERE alert_status = 'ALERT'
    """

    alert_count = hook.get_first(sql)[0]

    logger.info(f"ðŸš¨ Alert count: {alert_count}")

    return alert_count > 0

def build_emission_email(**context):
    """
    Build dynamic HTML email content using gold_emission_alerts table
    """

    hook = DatabricksSqlHook(
        databricks_conn_id="databricks_default"
    )

    sql = """
        SELECT
            country,
            region,
            year,
            total_co2_emissions_mt,
            threshold_value,
            alert_status
        FROM co2_emissions.default.gold_emission_alerts
        WHERE alert_status = 'ALERT'
        ORDER BY total_co2_emissions_mt DESC
    """

    records = hook.get_records(sql)

    # Safety check (should not occur due to ShortCircuitOperator)
    if not records:
        return """
        <h3>âœ… COâ‚‚ Emissions Status: Normal</h3>
        <p>No emission thresholds were breached.</p>
        """

    rows = ""
    for country, region, year, emission, threshold, status in records:
        rows += f"""
        <tr>
            <td>{country}</td>
            <td>{region}</td>
            <td>{year}</td>
            <td>{emission}</td>
            <td>{threshold}</td>
            <td style="color:red; font-weight:bold;">{status}</td>
        </tr>
        """

    html_content = f"""
    <h3>ðŸš¨ COâ‚‚ Emission Spike Alert</h3>

    <p>
        <b>Total Countries Triggered:</b> {len(records)} <br/>
        <b>Threshold Logic:</b> 95th Percentile (Global)
    </p>

    <table border="1" cellpadding="6" cellspacing="0"
           style="border-collapse:collapse;">
        <tr style="background-color:#f2f2f2;">
            <th>Country</th>
            <th>Region</th>
            <th>Year</th>
            <th>Total COâ‚‚ Emissions (MT)</th>
            <th>Threshold</th>
            <th>Status</th>
        </tr>
        {rows}
    </table>

    <br/>
    <h4>ðŸ“Š Power BI â€“ Active Emission Alerts</h4>
    <p>
    Below is a snapshot of the Power BI dashboard showing
    active COâ‚‚ emission alerts and trends.
    </p>

    <img src="cid:powerbi_alerts" width="700"/>

    <hr/>
    <p style="font-size:12px;color:gray;">
    Generated by Airflow Â· Databricks GOLD layer Â· {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}
    </p>
    """
    return html_content

def on_failure(context):
    logger.error(f"âŒ Task failed: {context['task_instance'].task_id}")

def on_success(context):
    logger.info(
        f"âœ… Pipeline completed in {context['task_instance'].duration}s"
    )

# ------------------------------------------------
# Default Args
# ------------------------------------------------
default_args = {
    "owner": "Nikhitha",
    "depends_on_past": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
    "on_failure_callback": on_failure,
    "on_success_callback": on_success,
}

# ------------------------------------------------
# DAG Definition
# ------------------------------------------------
with DAG(
    dag_id="co2_emissions_databricks_etl",
    description="Trigger Databricks Serverless Job for COâ‚‚ Emissions ETL",
    default_args=default_args,
    start_date=datetime(2024, 1, 1),
    schedule_interval=None,
    catchup=False,
    tags=["co2_emissions", "databricks", "serverless"],
) as dag:

    # 1ï¸âƒ£ Validate Inputs
    validate_task = PythonOperator(
        task_id="validate_inputs",
        python_callable=validate_inputs,
    )

    # 2ï¸âƒ£ Run Databricks Job
    run_co2_emissions_job = DatabricksRunNowOperator(
        task_id="run_co2_emissions_serverless_job",
        databricks_conn_id="databricks_default",
        job_id=166685760796878,
    )

    # 3ï¸âƒ£ Check Outputs
    check_outputs_task = PythonOperator(
        task_id="check_outputs",
        python_callable=check_outputs,
    )

    # 4ï¸âƒ£ Check Emission Alerts (Conditional)
    check_alerts_task = ShortCircuitOperator(
        task_id="check_emission_alerts",
        python_callable=check_emission_alerts,
    )
    # 5ï¸âƒ£ Prepare Email Content (Dynamic)
    prepare_email_task = PythonOperator(
        task_id="prepare_emission_email_content",
        python_callable=build_emission_email,
    )

    # 6ï¸âƒ£ Send Email Alert
    send_email_alert = EmailOperator(
        task_id="send_emission_alert_email",
        to=["bandelanikhithareddy5555@gmail.com"],
        subject="ðŸš¨ COâ‚‚ Emission Spike Alert",
        html_content="{{ ti.xcom_pull(task_ids='prepare_emission_email_content') }}",
        files=["/opt/airflow/logs/powerbi_emission_alerts.png"],
        mime_subtype="related",
    )


    # ----------------------------
    # Task Flow
    # ----------------------------
    validate_task \
        >> run_co2_emissions_job \
        >> check_outputs_task \
        >> check_alerts_task \
        >> prepare_email_task \
        >> send_email_alert